{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import json \n",
    "import os\n",
    "from Network.deepnetwork2 import ImitationNetwork\n",
    "from Memory.dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from Utils.utils import *\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch.nn as nn\n",
    "from DQNAgent import DQNAgent\n",
    "from ImitationAgent import ImitationAgent\n",
    "from DDQNAgent import DDQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer_Imitation = SummaryWriter('TensorBoard/Evaluate_Train_Imitation_TotalReward')\n",
    "writer_DQN = SummaryWriter('TensorBoard/Evaluate_Train_DQN_TotalReward')\n",
    "writer_DDQN = SummaryWriter('TensorBoard/Evaluate_Train_DDQN_TotalReward')\n",
    "writer_NFSP = SummaryWriter('TensorBoard/Evaluate_Train_NFSP_TotalReward')\n",
    "writer_NFSP_DDQN = SummaryWriter('TensorBoard/Evaluate_Train_NFSP_DDQN_TotalReward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Feeder3('Log/ImitationLog/Train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,batch_size = 256, num_workers = 20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n",
      "########## Loading checkpoint ##########\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#### Imitation Agent ####\n",
    "#########################\n",
    "\n",
    "agentImitation = ImitationAgent(alpha=0.001,\n",
    "                 input_dims=516,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, \n",
    "                 batch_size=32, replace=1000,\n",
    "                 checkpoint_dir='Models/Evaluate_Train/1stImitationAgent_OldData.ckpt',algo='Imitation',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentImitation.load_models()\n",
    "\n",
    "#########################\n",
    "####### DQN Agent #######\n",
    "#########################\n",
    "\n",
    "agentDQN = DQNAgent(gamma=0.1, epsilon=0.7, alpha=0.0001,replace_target_cnt=50,\n",
    "                 input_dims=INPUT_DIM,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, eps_min=0.1,\n",
    "                 batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                 checkpoint_dir='Models/Evaluate_Train', algo='DQNAgent',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentDQN.load_models()\n",
    "\n",
    "#########################\n",
    "####### DDQN Agent ######\n",
    "#########################\n",
    "\n",
    "agentDDQN = DDQNAgent(gamma=0.1, epsilon=0.7, alpha=0.0001,input_dim=INPUT_DIM,\n",
    "                     n_actions=NUM_ACTIONS, mem_size=50000, eps_min=0.1,\n",
    "                     batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                     chkpt_dir='Models/Evaluate_Train', algo='DDQNAgent',\n",
    "                     env_name='Quest_of_Divinity')\n",
    "agentDDQN.load_models()\n",
    "\n",
    "#########################\n",
    "#### DDQN NFSP Agent ####\n",
    "#########################\n",
    "\n",
    "agentImitation_DDQN_NFSP = ImitationAgent(alpha=0.001,\n",
    "                 input_dims=516,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, \n",
    "                 batch_size=32, replace=1000,\n",
    "                 checkpoint_dir='Models/Evaluate_Train/NFSP/1stImitationAgent_OldData.ckpt',algo='Imitation',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentImitation_DDQN_NFSP.load_models()\n",
    "\n",
    "agentDDQN_DDQN_NFSP = DDQNAgent(gamma=0.1, epsilon=0.7, alpha=0.0001,input_dim=INPUT_DIM,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, eps_min=0.1,\n",
    "                 batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                 chkpt_dir='Models/Evaluate_Train/NFSP/', algo='DDQNAgent',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentDDQN_DDQN_NFSP.load_models()\n",
    "\n",
    "#########################\n",
    "##### DQN NFSP Agent ####\n",
    "#########################\n",
    "\n",
    "agentImitation_DQN_NFSP = ImitationAgent(alpha=0.001,\n",
    "                 input_dims=516,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, \n",
    "                 batch_size=32, replace=1000,\n",
    "                 checkpoint_dir='Models/Evaluate_Train/NFSP/1stImitationAgent_OldData_DDQN.ckpt',algo='Imitation',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentImitation_DQN_NFSP.load_models()\n",
    "# torch.save(agentImitation.network.state_dict(), 'Models/NFSP/1stImitationAgent_OldData')\n",
    "\n",
    "agentDQN_DQN_NFSP = DQNAgent(gamma=0.1, epsilon=0.7, alpha=0.0001,replace_target_cnt=50,\n",
    "                 input_dims=INPUT_DIM,\n",
    "                 n_actions=NUM_ACTIONS, mem_size=50000, eps_min=0.1,\n",
    "                 batch_size=32, replace=1000, eps_dec=1e-5,\n",
    "                 checkpoint_dir='Models/Evaluate_Train/NFSP/', algo='DQNAgent',\n",
    "                 env_name='Quest_of_Divinity')\n",
    "agentDQN_DQN_NFSP.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score_imi = 0.0\n",
    "total_score_dqn = 0.0\n",
    "total_score_ddqn = 0.0\n",
    "total_score_nfsp = 0.0\n",
    "total_score_ddqn_nfsp = 0.0\n",
    "DEVICE = torch.device('cuda:0')\n",
    "\n",
    "score_DQN_NFSP = 0.0\n",
    "score_Imi_NFSP = 0.0\n",
    "num_DQN_NFSP = 0\n",
    "num_Imi_NFSP = 0\n",
    "nuy_DQN_NFSP = 0.5\n",
    "count_NFSP = 0\n",
    "\n",
    "score_DDQN_NFSP = 0.0\n",
    "score_Imi_DDQN_NFSP = 0.0\n",
    "num_DDQN_NFSP = 0\n",
    "num_Imi_DDQN_NFSP = 0\n",
    "nuy_DDQN_NFSP = 0.5\n",
    "count_DDQN_NFSP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kietcdx/LV/Agent_GitHub/ImitationAgent.py:101: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = crit(pred,torch.nn.functional.softmax(reward_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kietcdx/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/kietcdx/myenv/lib/python3.6/site-packages/ipykernel_launcher.py:124: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(5.6843e-14, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(0.0819, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(0.4394, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(2.6704e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(1.2343e-06, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(3.3244, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(2.1823, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(0.0071, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(1.3542e-07, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(4.8614, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(4.0636, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(9.3253e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(1.3280e-06, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(39.1936, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(0.1145, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(4.4363e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(2.0873e-06, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(0.9782, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(36.5841, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(2.0512e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "tensor(1.5943e-06, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(5.7300, device='cuda:1', grad_fn=<MseLossBackward>)\n",
      "tensor(24.7477, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "tensor(3.1410e-05, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n",
      "########## Saving checkpoint ##########\n"
     ]
    }
   ],
   "source": [
    "# for j in range(2):\n",
    "for i, data in enumerate(dataset.data):\n",
    "#     i = j + 313320\n",
    "    # Imitation \n",
    "    state_imi = torch.from_numpy(np.array(decode_state_old_test(data))).float()\n",
    "    state_device = state_imi.to(DEVICE)\n",
    "    idx = i\n",
    "    action_imi = torch.argmax(agentImitation.network.forward(state_device).detach().cpu())\n",
    "\n",
    "    reward_imi = Reward(data['player_board_card_info'],data['opponent_board_card_info'],data['player_hand_card_id'],data['opponent_life'],data['player_life'],data['player_gold'])\n",
    "    total_score_imi += reward_imi[action_imi]\n",
    "    writer_Imitation.add_scalar('Total Score Train',total_score_imi,i)\n",
    "\n",
    "\n",
    "    # DQN\n",
    "\n",
    "    state_dqn = decode_state_old_test(data)\n",
    "    reward_dqn = Reward(data['player_board_card_info'],data['opponent_board_card_info'],data['player_hand_card_id'],data['opponent_life'],data['player_life'],data['player_gold'])\n",
    "    state_dqn = T.tensor(state_dqn,dtype=T.float).to(torch.device('cuda:1'))\n",
    "    actions_dqn = agentDQN.q_eval.forward(state_dqn).detach().cpu()\n",
    "    action_dqn = T.argmax(actions_dqn).item()\n",
    "\n",
    "    total_score_dqn += reward_dqn[action_dqn]\n",
    "    writer_DQN.add_scalar('Total Score Train',total_score_dqn,i)\n",
    "    try:\n",
    "        next_state = decode_state_old_test(dataset.data[idx+1])\n",
    "        agentDQN.store(state_dqn,action_dqn,reward_dqn[action_dqn],next_state, False)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    agentDQN.learn()\n",
    "\n",
    "    # DQN NFSP\n",
    "\n",
    "    if count_NFSP > 5:\n",
    "        count_NFSP = 0\n",
    "        nuy_DQN_NFSP = 0.5 \n",
    "    dynamic = np.random.random()\n",
    "    if  dynamic < nuy_DQN_NFSP:\n",
    "        action,reward,state = agentDQN_DQN_NFSP.choose_action(data)\n",
    "        score = reward[action]\n",
    "        score_DQN_NFSP += score\n",
    "        num_DQN_NFSP += 1 \n",
    "        try:\n",
    "            next_state = decode_state_old_test(dataset.data[i+1])\n",
    "        except IndexError:\n",
    "            break        \n",
    "        agentDQN_DQN_NFSP.store(state,action,reward[action],next_state, False)\n",
    "        return_reward = [max(reward[:56])] +[max(reward[56:105])] + [max(reward[105:114])]  + [max(reward[114:177])] + [max(reward[177:289])] + [reward[289]]     \n",
    "        agentImitation_DQN_NFSP.store(state,action,return_reward)\n",
    "        agentImitation_DQN_NFSP.learn()\n",
    "        agentDQN_DQN_NFSP.learn()\n",
    "        total_score_nfsp += score\n",
    "\n",
    "    else:\n",
    "        action,sco,reward,state = agentImitation_DQN_NFSP.choose_action(data)\n",
    "        score_Imi_NFSP += sco\n",
    "        num_Imi_NFSP += 1\n",
    "        try:\n",
    "            next_state = decode_state_old_test(dataset.data[i+1])\n",
    "        except:\n",
    "            break \n",
    "#         agentImitation_DQN_NFSP.store(state,action,reward)\n",
    "        agentDQN_DQN_NFSP.store(state,action,sco,next_state, False)\n",
    "        agentDQN_DQN_NFSP.learn()\n",
    "#         agentImitation_DQN_NFSP.learn()\n",
    "        total_score_nfsp += sco\n",
    "\n",
    "    writer_NFSP.add_scalar('Total Score Train',total_score_nfsp,i)   \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        try:\n",
    "            smax = torch.nn.functional.softmax(torch.tensor([score_DQN_NFSP/num_DQN_NFSP,score_Imi_NFSP/num_Imi_NFSP]))\n",
    "        except ZeroDivisionError:\n",
    "            count_NFSP += 1\n",
    "            continue\n",
    "        nuy_DQN_NFSP = smax[0]\n",
    "        score_DQN_NFSP = 0.0\n",
    "        score_Imi_NFSP = 0.0\n",
    "        num_DQN_NFSP = 0\n",
    "        num_Imi_NFSP = 0\n",
    "\n",
    "\n",
    "    # DDQN NFSP\n",
    "    if count_DDQN_NFSP > 5:\n",
    "        count_DDQN_NFSP = 0\n",
    "        nuy_DDQN_NFSP = 0.5 \n",
    "    dynamic = np.random.random()\n",
    "    if  dynamic < nuy_DDQN_NFSP:\n",
    "        action,reward,state = agentDDQN_DDQN_NFSP.choose_action(data)\n",
    "        score = reward[action]\n",
    "        score_DDQN_NFSP += score\n",
    "        num_DDQN_NFSP += 1 \n",
    "        try:\n",
    "            next_state = decode_state_old_test(dataset.data[i+1])\n",
    "        except:\n",
    "            break\n",
    "        agentDDQN_DDQN_NFSP.store(state,action,reward[action],next_state, False)\n",
    "\n",
    "\n",
    "        return_reward = [max(reward[:56])] +[max(reward[56:105])] + [max(reward[105:114])]  + [max(reward[114:177])] + [max(reward[177:289])] + [reward[289]]     \n",
    "        agentImitation_DDQN_NFSP.store(state,action,return_reward)    \n",
    "\n",
    "        agentDDQN_DDQN_NFSP.learn()\n",
    "        agentImitation_DDQN_NFSP.learn()\n",
    "\n",
    "        total_score_ddqn_nfsp += score\n",
    "\n",
    "    else:\n",
    "        action,sco,reward,state = agentImitation_DDQN_NFSP.choose_action(data)\n",
    "        score_Imi_DDQN_NFSP += sco\n",
    "        num_Imi_DDQN_NFSP += 1\n",
    "        try:\n",
    "            next_state = decode_state_old_test(dataset.data[i+1])\n",
    "        except IndexError:\n",
    "            break \n",
    "#         agentImitation_DDQN_NFSP.store(state,action,reward)\n",
    "        agentDDQN_DDQN_NFSP.store(state,action,sco,next_state, False)\n",
    "        agentDDQN_DDQN_NFSP.learn()\n",
    "#         agentImitation_DDQN_NFSP.learn()\n",
    "        total_score_ddqn_nfsp += sco\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        try:\n",
    "            smax = torch.nn.functional.softmax(torch.tensor([score_DDQN_NFSP/num_DDQN_NFSP,score_Imi_DDQN_NFSP/num_Imi_DDQN_NFSP]))\n",
    "        except ZeroDivisionError:\n",
    "            count_DDQN_NFSP += 1  \n",
    "            continue\n",
    "        nuy_DDQN_NFSP = smax[0]\n",
    "        score_DDQN_NFSP = 0.0\n",
    "        score_Imi_DDQN_NFSP = 0.0\n",
    "        num_DDQN_NFSP = 0\n",
    "        num_Imi_DDQN_NFSP = 0\n",
    "\n",
    "    writer_NFSP_DDQN.add_scalar('Total Score Train',total_score_ddqn_nfsp,i)   \n",
    "\n",
    "    # DDQN \n",
    "    state_ddqn = decode_state_old_test(data)\n",
    "    reward_ddqn = Reward(data['player_board_card_info'],data['opponent_board_card_info'],data['player_hand_card_id'],data['opponent_life'],data['player_life'],data['player_gold'])\n",
    "    state_ddqn = T.tensor(state_ddqn,dtype=T.float).to(DEVICE)\n",
    "    actions_ddqn = agentDDQN.q_eval.forward(state_ddqn).detach().cpu()\n",
    "    action_ddqn = T.argmax(actions_ddqn).item()\n",
    "\n",
    "    total_score_ddqn += reward_ddqn[action_ddqn]\n",
    "    writer_DDQN.add_scalar('Total Score Train',total_score_ddqn,i)    \n",
    "    try:\n",
    "        next_state = decode_state_old_test(dataset.data[idx+1])\n",
    "        agentDDQN.store(state_ddqn,action_ddqn,reward_ddqn[action_ddqn],next_state, False)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    agentDDQN.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
